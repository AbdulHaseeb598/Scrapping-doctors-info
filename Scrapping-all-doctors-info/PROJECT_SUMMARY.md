# üöÄ PROJECT SUMMARY - Marham.pk Complete Database Scraper

## ‚úÖ PROJECT OVERVIEW

**Purpose:** Automated extraction of ALL doctors from ALL cities on Marham.pk into a single CSV database

**Technology:** Playwright-based browser automation (Chromium)

**Output:** Single CSV file with 18+ fields per doctor √ó hospital combination

---

## üìÅ FINAL FILES

### ‚úÖ KEEP THESE FILES:

```
‚úÖ scrape_doctors.py               ‚Üê MAIN SCRAPER (Playwright-based)
‚úÖ doctors_knowledge_base.csv      ‚Üê OUTPUT FILE (generated by scraper)
‚úÖ requirements.txt                ‚Üê Python dependencies
‚úÖ README.md                       ‚Üê Complete documentation
‚úÖ PROJECT_SUMMARY.md              ‚Üê This file (quick reference)
```

### üì¶ DEPENDENCIES:
```
playwright>=1.40.0
```

---

## üîÑ COMPLETE WORKFLOW

### **Step 1: Setup** (One-time)
```bash
# Navigate to directory
cd "Scrapping-doctors-info"

# Create virtual environment
python -m venv venv

# Activate environment
.\venv\Scripts\Activate.ps1

# Install dependencies
pip install -r requirements.txt

# Install Playwright browsers
playwright install chromium
```

### **Step 2: Configure** (Optional)
Edit `scrape_doctors.py`:
```python
# Line 13-17
OUTPUT_CSV = "doctors_knowledge_base.csv"  # Output filename
HEADLESS = False                           # False for first run (Cloudflare)
CITY_LIMIT: Optional[int] = None           # None = all cities, 2 = test
DELAY_MIN, DELAY_MAX = 0.8, 2.0           # Random delay (seconds)
MAX_PAGES_PER_CITY = 8                    # Pages to scrape per city
```

### **Step 3: Run Scraper**
```bash
python scrape_doctors.py
```

### **Step 4: Monitor Progress**
```
Discovered 50 cities; already scraped 0.

=== Processing Karachi ===
Saved 120 rows for Karachi

=== Processing Lahore ===
Saved 95 rows for Lahore
...
```

### **Step 5: Resume Capability**
- Script can be interrupted anytime
- Run again ‚Üí automatically skips completed cities
- Continue from where it left off

### **Step 6: Output**
- **File:** `doctors_knowledge_base.csv`
- **Size:** 2-5 MB (2,500-7,500+ rows)
- **Format:** UTF-8 encoded CSV with headers

---

## üìä DATA PIPELINE

```
START
  ‚Üì
Launch Playwright Browser (Chromium)
  ‚Üì
Visit: https://www.marham.pk/doctors
  ‚Üì
CITY DISCOVERY
  ‚îî‚îÄ Extract all city links
  ‚îî‚îÄ Filter valid cities (/doctors/{city} pattern)
  ‚îî‚îÄ Output: List of 50+ cities
  ‚Üì
RESUME CHECK
  ‚îî‚îÄ Read existing CSV
  ‚îî‚îÄ Extract already scraped cities
  ‚îî‚îÄ Skip completed cities
  ‚Üì
FOR EACH CITY (not yet scraped):
  ‚Üì
  PAGINATION LOOP (up to MAX_PAGES_PER_CITY):
    ‚Üì
    Visit city page
    ‚Üì
    FOR EACH DOCTOR CARD:
      ‚Üì
      EXTRACT BASIC INFO:
        - Name (h3)
        - Specialization (p.text-sm)
        - Qualification (p.text-sm)
        - Image URL (picture > source or img)
        - Profile URL (a.dr_profile_opened_from_listing)
      ‚Üì
      EXTRACT METRICS (div.col-4 blocks):
        - Experience (regex: /\d+ yrs?/)
        - Satisfaction Rate (regex: /\d+%/)
        - Reviews (regex: /\d{1,3}(,\d{3})*/)
      ‚Üì
      EXTRACT AREAS OF INTEREST:
        - Tags (span.chips-highlight, span.chips)
      ‚Üì
      VISIT PROFILE PAGE (for schedules):
        - Extract weekly availability tables
        - Cache results (avoid re-fetching)
        - Build: {hospital_name: "Mon: 09:00 AM - 05:00 PM; ..."}
      ‚Üì
      FOR EACH HOSPITAL CARD (div.product-card):
        ‚Üì
        EXTRACT HOSPITAL DATA (data- attributes):
          - Hospital name (data-hospitalname)
          - Hospital address (data-hospitaladdress)
          - Hospital city (data-hospitalcity)
          - Consultation fee (data-amount)
          - Type (data-hospitaltype: 2 = Video)
        ‚Üì
        MATCH SCHEDULE:
          - Find best matching schedule from profile
          - Use hospital name token matching
        ‚Üì
        BUILD CSV ROW (18 fields):
          - Doctor info + Hospital info + Availability
        ‚Üì
        ADD TO RESULTS
      ‚Üì
    ‚Üì
    Check for "Next Page" button
    ‚Üì
    If exists: Navigate to next page
    ‚Üì
  ‚Üì
  SAVE CITY DATA:
    - Append all rows to CSV
    - Data persisted immediately
  ‚Üì
  Random delay (DELAY_MIN to DELAY_MAX)
  ‚Üì
  Next city
  ‚Üì
ALL CITIES COMPLETE
  ‚Üì
Close browser
  ‚Üì
Print summary
  ‚Üì
END
```

---

## üìã OUTPUT CSV STRUCTURE

### **18 Fields:**

| Field | Description | Example |
|-------|-------------|---------|
| `city` | Source city | Lahore |
| `name` | Doctor name | Dr. Aisha Khan |
| `specialization` | Medical specialty | Gynecologist |
| `qualification` | Degrees | MBBS, FCPS |
| `experience` | Years | 15 yrs |
| `satisfaction_rate` | Patient satisfaction | 98% |
| `reviews` | Review count | 250 |
| `areas_of_interest` | Subspecialties | Pregnancy Care, High-Risk Pregnancy |
| `consultation_type` | Hospital/Video | Hospital |
| `hospital_name` | Hospital/clinic | XYZ Medical Center |
| `hospital_address` | Street address | Model Town, Lahore |
| `hospital_city` | City | Lahore |
| `complete address` | Additional notes | Available Mon-Fri |
| `availability_schedule` | Weekly timings | Mon: 09:00 AM - 05:00 PM; Tue: ... |
| `fee` | Consultation fee | Rs. 2500 |
| `profile_url` | Doctor profile link | https://www.marham.pk/doctors/... |
| `image_url` | Doctor photo | https://cdn.marham.pk/images/... |
| `raw_source_url` | Source listing URL | https://www.marham.pk/doctors/lahore |

### **Sample CSV Row:**
```csv
Lahore,Dr. Aisha Khan,Gynecologist,"MBBS, FCPS",15 yrs,98%,250,"Pregnancy Care, High-Risk Pregnancy",Hospital,XYZ Medical Center,"Model Town, Lahore",Lahore,"Available Mon-Fri","Mon: 09:00 AM - 05:00 PM; Tue: 09:00 AM - 05:00 PM",Rs. 2500,https://www.marham.pk/doctors/lahore/gynecologist/dr-aisha-khan-12345,https://cdn.marham.pk/images/dr-aisha.jpg,https://www.marham.pk/doctors/lahore
```

---

## üîß KEY FUNCTIONS

### **Main Functions:**

| Function | Purpose | Key Logic |
|----------|---------|-----------|
| `main()` | Entry point | Coordinates entire scraping workflow |
| `discover_city_links()` | Find cities | Extracts all `/doctors/{city}` links |
| `scrape_city_with_pagination()` | City scraper | Handles pagination (up to 8 pages) |
| `extract_doctors_from_city_page()` | Page scraper | Extracts all doctor cards from one page |
| `extract_availability_from_profile()` | Schedule extractor | Visits profile, extracts weekly tables |
| `extract_label_values()` | Metrics parser | Classifies experience/satisfaction/reviews |
| `match_schedule_for_hospital()` | Schedule matcher | Fuzzy match hospital name to schedule |

### **Helper Functions:**

| Function | Purpose |
|----------|---------|
| `apply_stealth()` | Add anti-detection scripts (webdriver, plugins) |
| `normalise_href()` | Convert relative URLs to absolute |
| `inner_text_safe()` | Safely extract text from elements |
| `classify_line_for_metric()` | Identify metric type using regex |
| `is_reviews_candidate()` | Check if text is review count |
| `append_rows()` | Write rows to CSV (append mode) |
| `read_scraped_cities()` | Read already processed cities |
| `rnd_sleep()` | Random delay between requests |

---

## üéØ CONFIGURATION OPTIONS

### **Test Run (2 cities only):**
```python
HEADLESS = False        # Visible browser
CITY_LIMIT = 2          # Only 2 cities
MAX_PAGES_PER_CITY = 3  # 3 pages per city
```

### **Production Run (all cities):**
```python
HEADLESS = True         # Headless mode (after Cloudflare bypass)
CITY_LIMIT = None       # All cities
MAX_PAGES_PER_CITY = 8  # 8 pages per city
```

### **Slower/Polite Scraping:**
```python
DELAY_MIN, DELAY_MAX = 2.0, 5.0  # 2-5 second delays
```

### **Faster Scraping:**
```python
DELAY_MIN, DELAY_MAX = 0.5, 1.0  # 0.5-1 second delays
```

---

## ‚ö° PERFORMANCE METRICS

### **Timing Estimates:**
- **City discovery:** 5-10 seconds
- **Single doctor card:** 1-2 seconds
- **Profile page (schedule):** 2-4 seconds
- **Per city (50 doctors, 8 pages):** 10-15 minutes
- **Full database (50 cities):** 8-12 hours

### **Expected Output:**
- **Cities:** 40-60 cities
- **Doctors:** 2,000-5,000 unique doctors
- **Rows:** 2,500-7,500+ rows (doctors √ó hospitals)
- **CSV Size:** 2-5 MB

---

## üêõ TROUBLESHOOTING GUIDE

### **1. Cloudflare Challenge**
**Problem:** Browser shows "Checking your browser..."

**Solution:**
```python
# Line 14
HEADLESS = False  # First run: visible browser
```
- Manually solve challenge if needed
- After success, set `HEADLESS = True`

### **2. Empty CSV**
**Problem:** No data extracted

**Solutions:**
- Check internet connection
- Run with `HEADLESS = False`
- Verify https://www.marham.pk/doctors is accessible

### **3. Playwright Not Installed**
**Solution:**
```bash
pip install playwright
playwright install chromium
```

### **4. Script Interrupted**
**Solution:**
- Just run again!
- Automatically skips completed cities
- Resumes from where it left off

### **5. Timeout Errors**
**Solutions:**
- Increase timeout values (line 102, 136, etc.)
- Reduce `MAX_PAGES_PER_CITY`
- Increase `DELAY_MIN` and `DELAY_MAX`

---

## üîí BEST PRACTICES

### **Responsible Scraping:**
- ‚úÖ Use reasonable delays (0.8+ seconds)
- ‚úÖ Run during off-peak hours
- ‚úÖ Don't run multiple instances
- ‚úÖ Respect website terms of service

### **Data Quality:**
- ‚úÖ Verify extracted data
- ‚úÖ Check for null/empty values
- ‚úÖ Cross-reference critical information

### **Maintenance:**
- ‚úÖ Keep Playwright updated
- ‚úÖ Monitor for HTML structure changes
- ‚úÖ Re-run `playwright install` after updates

---

## üìä COMPARISON: Two Projects

| Feature | **scrape_doctors.py** (This) | **marham_crawler2.py** (Other) |
|---------|------------------------------|-------------------------------|
| **Scope** | ALL doctors from ALL cities | Specific query search |
| **Automation** | Fully automated | Requires user input |
| **Output** | Single CSV database | Individual JSON files |
| **Browser** | Playwright (Chromium) | Crawl4AI (async) |
| **Resume** | Automatic (skips scraped) | Manual re-run |
| **Reviews** | Not included | Optional with LLM summary |
| **Use Case** | Complete database | Targeted search |
| **Run Time** | 8-12 hours (full) | 30-60 seconds per query |
| **Data Volume** | 2,500-7,500+ rows | 1 doctor per run |

---

## üéØ USE CASES

### **1. Healthcare Database**
- Medical directories
- Appointment booking systems
- Doctor finder applications

### **2. Market Research**
- Doctor distribution analysis
- Pricing trends across cities
- Specialty availability mapping

### **3. Data Analytics**
- Average fees by specialty
- Experience vs satisfaction correlation
- Geographic distribution analysis

### **4. System Integration**
- Import to SQL databases
- Load to NoSQL (MongoDB)
- Analytics tools (Tableau, Power BI)

---

## üìù DATA IMPORT EXAMPLES

### **Python Pandas:**
```python
import pandas as pd
df = pd.read_csv('doctors_knowledge_base.csv')
print(df.head())
print(f"Total rows: {len(df)}")
```

### **PostgreSQL:**
```sql
COPY doctors_knowledge_base
FROM '/path/to/doctors_knowledge_base.csv'
DELIMITER ','
CSV HEADER;
```

### **Excel/Google Sheets:**
- File ‚Üí Import ‚Üí CSV
- Select file
- Delimiter: Comma
- Encoding: UTF-8

---

## üö¶ QUICK START CHECKLIST

- [ ] Navigate to `Scrapping-doctors-info` directory
- [ ] Create virtual environment: `python -m venv venv`
- [ ] Activate: `.\venv\Scripts\Activate.ps1`
- [ ] Install dependencies: `pip install -r requirements.txt`
- [ ] Install browsers: `playwright install chromium`
- [ ] Configure `scrape_doctors.py` (optional):
  - [ ] Set `HEADLESS = False` (first run)
  - [ ] Set `CITY_LIMIT = 2` (test run)
- [ ] Run: `python scrape_doctors.py`
- [ ] Check output: `doctors_knowledge_base.csv`

---

## üìà EXPECTED CONSOLE OUTPUT

```
Discovered 50 cities; already scraped 0.

=== Processing Karachi ===
Saved 120 rows for Karachi

=== Processing Lahore ===
Saved 95 rows for Lahore

=== Processing Islamabad ===
Saved 78 rows for Islamabad

=== Processing Rawalpindi ===
Saved 65 rows for Rawalpindi

...

‚úÖ Done. Total saved this run: 2850. File: doctors_knowledge_base.csv
```

---

## üèÜ PROJECT STATUS

| Component | Status | Notes |
|-----------|--------|-------|
| City Discovery | ‚úÖ Complete | Auto-discovers all cities |
| Pagination | ‚úÖ Complete | Up to 8 pages per city |
| Doctor Extraction | ‚úÖ Complete | All card fields extracted |
| Profile Scraping | ‚úÖ Complete | Weekly schedules extracted |
| Hospital Parsing | ‚úÖ Complete | Multi-hospital support |
| Schedule Matching | ‚úÖ Complete | Fuzzy matching algorithm |
| CSV Export | ‚úÖ Complete | Incremental saves |
| Resume Capability | ‚úÖ Complete | Auto-skip completed cities |
| Stealth Mode | ‚úÖ Complete | Anti-detection scripts |
| Error Handling | ‚úÖ Complete | Try-catch on all operations |

---

## üë®‚Äçüíª AUTHOR

**Abdul Haseeb**
- GitHub: [@AbdulHaseeb598](https://github.com/AbdulHaseeb598)
- Repository: [Scrapping-doctors-info](https://github.com/AbdulHaseeb598/Scrapping-doctors-info)

**Project Context:** CUI Internship - Web Scraping & Data Extraction

---

## üìÖ VERSION INFO

**Version:** 1.0  
**Status:** Production Ready ‚úÖ  
**Last Updated:** October 24, 2025  
**Technology:** Playwright + Python 3.8+

---

**üéâ PROJECT COMPLETE! Ready to scrape complete Marham.pk doctors database!**

Run the scraper and build a comprehensive healthcare database with 2,500+ doctor records!

---

*For detailed documentation, see README.md*  
*For questions, open an issue on GitHub*
